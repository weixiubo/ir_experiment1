# 稀疏检索系统使用指南

## 目录

1. [环境准备](#环境准备)
2. [快速开始](#快速开始)
3. [详细使用说明](#详细使用说明)
4. [常见问题](#常见问题)
5. [高级功能](#高级功能)

## 环境准备

### 1. 激活conda环境

```bash
conda activate ir_experiment
```

### 2. 验证环境

```bash
python -c "import jieba; import chardet; import matplotlib; from docx import Document; print('环境检查通过！')"
```

如果提示缺少包，请安装：

```bash
pip install jieba chardet matplotlib python-docx
```

## 快速开始

### 一键运行所有分析

```bash
# 1. 数据统计分析
python data_analysis.py

# 2. 构建检索索引
python sparse_retrieval_system.py

# 3. 分词统计分析
python tokenization_analysis.py

# 4. 测试检索系统
python test_retrieval.py

# 5. 生成实验报告
python generate_report.py
python create_word_report.py
```

### 查看结果

- **数据统计**：`dataset_statistics.json`
- **分词统计**：`tokenization_statistics.json`
- **检索测试**：`retrieval_test_results.json`
- **可视化图表**：`*.png`
- **实验报告**：`第一次实验报告_稀疏检索算法实现.docx`

## 详细使用说明

### 1. 数据分析 (data_analysis.py)

**功能**：统计数据集的基本信息

**运行**：
```bash
python data_analysis.py
```

**输出**：
- 控制台：显示统计摘要
- `dataset_statistics.json`：详细统计数据

**主要统计指标**：
- 总文档数
- 总字符数
- 各类别文档数量
- 各类别平均文档长度

### 2. 构建检索索引 (sparse_retrieval_system.py)

**功能**：构建BM25检索索引

**运行**：
```bash
python sparse_retrieval_system.py
```

**参数说明**：
- `max_docs_per_category`：每个类别加载的最大文档数（默认50）
- 可以修改代码中的参数来调整

**输出**：
- `retrieval_index.pkl`：检索索引文件（约22MB）

**索引内容**：
- 倒排索引
- 文档长度索引
- 词汇表
- 文档频率统计

### 3. 分词分析 (tokenization_analysis.py)

**功能**：分析分词效果和统计信息

**运行**：
```bash
python tokenization_analysis.py
```

**输出**：
- 控制台：分词演示和统计摘要
- `tokenization_statistics.json`：详细分词统计
- `global_top_words.png`：全局高频词图表
- `category_vocabulary_size.png`：各类别词汇量对比
- `category_avg_length.png`：各类别平均文档长度

**分词演示**：
脚本会展示4个示例文本的分词效果，包括：
- 无停用词过滤的分词结果
- 停用词过滤后的分词结果
- 关键词提取结果

### 4. 测试检索系统 (test_retrieval.py)

**功能**：测试检索系统性能

**运行**：
```bash
python test_retrieval.py
```

**测试查询**：
系统会自动运行15个预设查询，涵盖不同领域：
- 计算机网络技术
- 人工智能深度学习
- 环境保护污染治理
- 经济发展市场改革
- 体育运动足球比赛
- 医疗健康疾病治疗
- 航天技术卫星发射
- 农业生产粮食种植
- 法律法规司法制度
- 教育改革学校发展
- 历史文化传统习俗
- 文学艺术诗歌创作
- 军事国防武器装备
- 政治外交国际关系
- 能源开发电力工业

**输出**：
- 控制台：每个查询的检索结果
- `retrieval_test_results.json`：详细测试结果

**交互式检索**：
测试完成后，可以选择进入交互式检索模式，输入自定义查询。

### 5. 生成实验报告

**步骤1：生成报告内容**
```bash
python generate_report.py
```

输出：`report_content.json`

**步骤2：创建Word文档**
```bash
python create_word_report.py
```

输出：`第一次实验报告_稀疏检索算法实现.docx`

## 常见问题

### Q1: 索引文件太大怎么办？

**A**: 可以减少每个类别加载的文档数量。修改 `sparse_retrieval_system.py` 中的参数：

```python
retrieval_system.load_documents('复旦中文文本分类语料库', max_docs_per_category=20)
```

### Q2: 检索速度慢怎么办？

**A**: 
1. 确保已经构建并保存了索引
2. 使用 `load_index()` 而不是重新构建
3. 减少候选文档数量

### Q3: 中文显示乱码怎么办？

**A**: 
1. 确保终端支持UTF-8编码
2. 检查数据文件编码
3. 脚本已包含自动编码检测功能

### Q4: 可视化图表中文显示为方框？

**A**: 
1. 安装中文字体（如SimHei）
2. 或者修改 `tokenization_analysis.py` 中的字体设置

### Q5: 如何修改BM25参数？

**A**: 在 `sparse_retrieval_system.py` 中修改：

```python
self.k1 = 1.5  # 调整词频饱和度
self.b = 0.75  # 调整文档长度归一化
```

## 高级功能

### 自定义查询

```python
from sparse_retrieval_system import ChineseTokenizer, SparseRetrievalSystem

# 加载系统
tokenizer = ChineseTokenizer(use_stopwords=True)
retrieval_system = SparseRetrievalSystem(tokenizer)
retrieval_system.load_index('retrieval_index.pkl')

# 自定义查询
query = "你的查询内容"
results = retrieval_system.search(query, top_k=10)

# 处理结果
for rank, (doc_id, score, doc_info) in enumerate(results, 1):
    print(f"{rank}. {doc_info['filename']} (分数: {score:.4f})")
    print(f"   类别: {doc_info['category']}")
    print(f"   预览: {doc_info['text'][:100]}...")
```

### 批量查询

```python
queries = [
    "查询1",
    "查询2",
    "查询3"
]

for query in queries:
    results = retrieval_system.search(query, top_k=5)
    print(f"\n查询: {query}")
    for rank, (doc_id, score, doc_info) in enumerate(results, 1):
        print(f"  {rank}. {doc_info['filename']} ({score:.4f})")
```

### 添加停用词

```python
# 修改 ChineseTokenizer 类中的停用词集合
tokenizer = ChineseTokenizer(use_stopwords=True)
tokenizer.stopwords.add('新增停用词')
```

### 自定义分词词典

```python
import jieba

# 添加自定义词汇
jieba.add_word('自定义词汇')

# 删除词汇
jieba.del_word('要删除的词')

# 调整词频
jieba.suggest_freq('词汇', True)
```

### 导出检索结果

```python
import json

results = retrieval_system.search(query, top_k=10)

# 转换为可序列化格式
export_data = []
for rank, (doc_id, score, doc_info) in enumerate(results, 1):
    export_data.append({
        'rank': rank,
        'doc_id': doc_id,
        'score': score,
        'category': doc_info['category'],
        'filename': doc_info['filename']
    })

# 保存为JSON
with open('search_results.json', 'w', encoding='utf-8') as f:
    json.dump(export_data, f, ensure_ascii=False, indent=2)
```

## 性能优化建议

### 1. 索引优化

- 使用pickle序列化索引，避免重复构建
- 对于大规模数据集，考虑使用数据库存储索引

### 2. 检索优化

- 限制候选文档数量
- 使用缓存存储热门查询结果
- 并行处理多个查询

### 3. 内存优化

- 分批加载文档
- 使用生成器处理大文件
- 及时释放不需要的数据

## 技术支持

如有问题，请查看：
- `README.md` - 项目说明
- `项目完成总结.md` - 项目总结
- 代码注释 - 详细的实现说明

---

**最后更新**：2025年10月21日

